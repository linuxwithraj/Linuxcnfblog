How to Install and Configure 2 Node Ceph Storage on CentOS 7
Ceph Storage is a free and open source widely used software defined storage solution which provides file system storage, object storage and block level centralize storage. 
This article describes How to Install and Configure 2 Node Ceph Storage on CentOS 7

Step 1. Hostname Configuration: Set host name on all the Ceph cluster nodes:

[root@localhost ~]# hostnamectl set-hostname LC-Ceph-MGMT;exec bash
[root@lc-ceph-mgmt ~]#

[root@localhost ~]# hostnamectl set-hostname LC-Storage1;exec bash
[root@lc-storage1 ~]#

hostnamectl set-hostname LC-Storage2;exec bash
[root@lc-storage2 ~]#

[root@lc-ceph-mgmt ~]# vi /etc/hosts
...................
192.168.30.30   lc-ceph-mgmt
192.168.30.23   lc-storage1
192.168.30.7	lc-storage2
[root@lc-ceph-mgmt ~]#
 
[root@lc-storage1 ~]# vi /etc/hosts
...................
192.168.30.30   lc-ceph-mgmt
192.168.30.23   lc-storage1
192.168.30.7	lc-storage2
[root@lc-storage1 ~]#

[root@lc-storage2 ~]# vi /etc/hosts
...................
192.168.30.30   lc-ceph-mgmt
192.168.30.23   lc-storage1
192.168.30.7	lc-storage2
[root@lc-storage2 ~]#

Step 2. NTP Configuration: 

Step 2. User creation: Create a user with sudo access on all storage node for deployment and :

[root@lc-storage1 ~]# useradd ceph; passwd ceph
Changing password for user ceph.
New password:
BAD PASSWORD: The password is shorter than 8 characters
Retype new password:
passwd: all authentication tokens updated successfully.
[root@lc-storage1 ~]# echo "ceph ALL = (root) NOPASSWD:ALL" > /etc/sudoers.d/ceph;chmod 0440 /etc/sudoers.d/ceph
[root@lc-storage1 ~]#

[root@lc-storage2 ~]# useradd ceph; passwd ceph
Changing password for user ceph.
New password:
BAD PASSWORD: The password is shorter than 8 characters
Retype new password:
passwd: all authentication tokens updated successfully.
[root@lc-storage2 ~]# echo "ceph ALL = (root) NOPASSWD:ALL" > /etc/sudoers.d/ceph;chmod 0440 /etc/sudoers.d/ceph
[root@lc-storage2 ~]#

Step 3. Passwordless authentication: Follow the article How to set password less authention from Node1 to all other Cluster nodes:

Generate random SSH RSA key on node 1:

[root@lc-ceph-mgmt ~]# ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:lzoZQ8du8DExL+A4w9WHbXyvkEgB0L0Zg9x1nr8hVzI root@lc-ceph-mgmt
The key's randomart image is:
+---[RSA 2048]----+
|      .++*+*. .  |
|     . ++oO+=o.. |
|      = +.*O.oE..|
|       + =+=o  +o|
|        S *  o +.|
|         B    + o|
|        +      . |
|         .       |
|                 |
+----[SHA256]-----+
[root@lc-ceph-mgmt ~]#

Copy the key to all other nodes:

[[root@lc-ceph-mgmt ~]# ssh-copy-id ceph@lc-storage1
/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"
...................
/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
ceph@lc-storage1's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'ceph@lc-storage1'"
and check to make sure that only the key(s) you wanted were added.

[root@lc-ceph-mgmt ~]#

[root@lc-ceph-mgmt ~]# ssh-copy-id ceph@lc-storage2
/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"
....................
/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
ceph@lc-storage2's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'ceph@lc-storage2'"
and check to make sure that only the key(s) you wanted were added.

[root@lc-ceph-mgmt ~]#


Put the below entris on ~/.ssh/config file to use ceph user as default for login on other cluster nodes from Node1:

[root@lc-ceph-mgmt ~]# vi ~/.ssh/config
Host lc-storage1
   Hostname lc-storage1
   User ceph
Host lc-storage2
   Hostname lc-storage2
   User ceph
[root@lc-ceph-mgmt ~]# 

Change permition of the file:

[root@lc-ceph-mgmt ~]# chmod 644 ~/.ssh/config
[root@lc-ceph-mgmt ~]#

Verify the login status on all the Cluster node from Admin Node1:

[root@lc-ceph-mgmt ~]# ssh lc-storage1
[ceph@lc-storage1 ~]$ exit
logout
Connection to lc-storage1 closed.
[root@lc-ceph-mgmt ~]# ssh lc-storage2
[ceph@lc-storage2 ~]$ exit
logout
Connection to lc-storage2 closed.
[root@lc-ceph-mgmt ~]#

Step 4. Repository Configuration: Follow the article How to Setup EPEL Repository on CentOS 7 to setup EPEL repository on Admin Node1 as some Ceph dependencies will be installed from it.

Ceph Repository Configuration: Ceph packages can be installed by its offical repository. Configure the Ceph repository as below on Ceph-MGMT Node:

[root@lc-ceph-mgmt ~]# yum install https://download.ceph.com/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm yum-plugin-priorities -y
Loaded plugins: fastestmirror
..................
Installed:
  ceph-release.noarch 0:1-1.el7                              yum-plugin-priorities.noarch 0:1.1.31-52.el7

Complete!
[root@lc-ceph-mgmt ~]#

Step 5. Ceph deploy: Install ceph deploy tool on Ceph-MGMT Node to deploy Ceph Storage Cluster on lc-storage1 and lc-storage2 nodes:

[root@lc-ceph-mgmt ~]# yum install ceph-deploy -y
Loaded plugins: fastestmirror
.............
Installed:
  ceph-deploy.noarch 0:2.0.1-0

Complete!
[root@lc-ceph-mgmt ~]#

Step 6. Ceph Storage Cluster Deployment: Run the bellow commands to install Ceph Cluster packages:

[root@lc-ceph-mgmt ~]# mkdir ~/ceph-cluster
[root@lc-ceph-mgmt ~]# cd ~/ceph-cluster
[root@lc-ceph-mgmt ceph-cluster]#

Run the below command to Generate configuration file:

[root@lc-ceph-mgmt ceph-cluster]# ceph-deploy new lc-storage1 lc-storage2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy new lc-storage1 lc-storage2
..............
[ceph_deploy.new][DEBUG ] Creating a random mon key...
[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[root@lc-ceph-mgmt ceph-cluster]# 
-----------------------------------------
Put the follwoing entry on the Ceph configuration file ~/ceph-cluster/ceph.conf:

[root@lc-node1 ceph-cluster]# vi ceph.conf
[global]
.............
osd pool default size = 2
public network = 192.168.30.0/24
[root@lc-node1 ceph-cluster]#
----------------------------------------
Run the following command to install Ceph packages on all the nodes:

[root@lc-ceph-mgmt ceph-cluster]# ceph-deploy install lc-storage1 lc-storage2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy install lc-storage1 lc-storage2
..............
[lc-storage1][DEBUG ] Complete!
[lc-storage1][INFO  ] Running command: sudo ceph --version
[lc-storage1][DEBUG ] ceph version 13.2.6 (7b695f835b03642f85998b2ae7b6dd093d9fbce4) mimic (stable)
[ceph_deploy.install][DEBUG ] Detecting platform for host lc-storage2 ...
[lc-storage2][DEBUG ] connection detected need for sudo
[lc-storage2][DEBUG ] connected to host: lc-storage2
..............
[lc-storage2][DEBUG ] Complete!
[lc-storage2][INFO  ] Running command: sudo ceph --version
[lc-storage2][DEBUG ] ceph version 13.2.6 (7b695f835b03642f85998b2ae7b6dd093d9fbce4) mimic (stable)
[root@lc-ceph-mgmt ceph-cluster]#

Step 7. Run the following command add the initial monitor and gather the keys:

[root@lc-ceph-mgmt ceph-cluster]# ceph-deploy mon create-initial
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy mon create-initial
..............
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp5Mr6KO
[root@lc-ceph-mgmt ceph-cluster]#

Step 8. Copy Admin Key: Run the follwoing command to copy the configuration and admin key Ceph Admin Node1 to all other cluster nodes.

[root@lc-ceph-mgmt ceph-cluster]# ceph-deploy admin lc-storage1 lc-storage2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy admin lc-storage1 lc-storage2
................
[lc-storage2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[root@lc-ceph-mgmt ceph-cluster]#

Step 9. Run the follwoing command to deploy manager daemon:

[root@lc-ceph-mgmt ceph-cluster]# ceph-deploy mgr create lc-storage1 lc-storage2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy mgr create lc-storage1 lc-storage2
..................
[lc-storage2][INFO  ] Running command: sudo systemctl start ceph-mgr@lc-storage2
[lc-storage2][INFO  ] Running command: sudo systemctl enable ceph.target
[root@lc-ceph-mgmt ceph-cluster]#

Step 9. Run the following command list out hard disks on Storage nodes to create OSDs:

[root@lc-ceph-mgmt ceph-cluster]# ceph-deploy disk list lc-storage1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy disk list lc-storage1
.....................
[lc-storage1][INFO  ] Running command: sudo fdisk -l
[lc-storage1][INFO  ] Disk /dev/vda: 42.9 GB, 42949672960 bytes, 83886080 sectors
[lc-storage1][INFO  ] Disk /dev/vdb: 5368 MB, 5368709120 bytes, 10485760 sectors
[root@lc-ceph-mgmt ceph-cluster]#

[root@lc-ceph-mgmt ceph-cluster]# ceph-deploy disk list lc-storage2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy disk list lc-storage2
....................
[lc-storage2][INFO  ] Running command: sudo fdisk -l
[lc-storage2][INFO  ] Disk /dev/vda: 42.9 GB, 42949672960 bytes, 83886080 sectors
[lc-storage2][INFO  ] Disk /dev/vdb: 5368 MB, 5368709120 bytes, 10485760 sectors
[root@lc-ceph-mgmt ceph-cluster]#

Step 10. Run the following commonds to create OSD and add disks for Ceph storage cluster (Make sure all disks should be unused, all the data will be erased permanantly):


[root@lc-ceph-mgmt ceph-cluster]# ceph-deploy osd create --data /dev/vdb lc-storage1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy osd create --data /dev/vdb lc-storage1
....................
[lc-storage1][DEBUG ] --> ceph-volume lvm create successful for: /dev/vdb
[lc-storage1][INFO  ] checking OSD status...
[lc-storage1][DEBUG ] find the location of an executable
[lc-storage1][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host lc-storage1 is now ready for osd use.
[root@lc-ceph-mgmt ceph-cluster]#

[root@lc-ceph-mgmt ceph-cluster]# ceph-deploy osd create --data /dev/vdb lc-storage2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy osd create --data /dev/vdb lc-storage2
....................
[lc-storage2][DEBUG ] --> ceph-volume lvm create successful for: /dev/vdb
[lc-storage2][INFO  ] checking OSD status...
[lc-storage2][DEBUG ] find the location of an executable
[lc-storage2][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host lc-storage2 is now ready for osd use.
[root@lc-ceph-mgmt ceph-cluster]#

Step 11. Validate installation: Login to any once LC-Storage Node and Run following command to check and verify the installation and configuration status of Ceph Storage Cluster.

[root@lc-storage1 ~]# ceph
ceph> health
HEALTH_OK

ceph> status
  cluster:
    id:     143918ca-8ba4-4176-8636-9ad0a6d004d8
    health: HEALTH_OK

  services:
    mon: 2 daemons, quorum lc-storage2,lc-storage1
    mgr: lc-storage1(active), standbys: lc-storage2
    osd: 2 osds: 2 up, 2 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0  objects, 0 B
    usage:   2.0 GiB used, 8.0 GiB / 10 GiB avail
    pgs:


ceph> q
[root@lc-storage1 ~]#

Done!!! Ceph Storage Cluster configuration has been done.



 










